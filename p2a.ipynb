{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixelhop import Pixelhop\n",
    "import numpy as np\n",
    "from skimage.util import view_as_windows\n",
    "import pickle\n",
    "from skimage.measure import block_reduce\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
    "import warnings, gc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Preprocess\n",
    "N_Train_Reduced = 10000    # 10000\n",
    "N_Train_Full = 60000     # 50000\n",
    "N_Test = 10000            # 10000\n",
    "\n",
    "BS = 10000 # batch size\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    shuffle_idx = np.random.permutation(y.size)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def select_balanced_subset(images, labels, use_num_images):\n",
    "    '''\n",
    "    select equal number of images from each classes\n",
    "    '''\n",
    "    num_total, H, W, C = images.shape\n",
    "    num_class = np.unique(labels).size\n",
    "    num_per_class = int(use_num_images / num_class)\n",
    "\n",
    "    # Shuffle\n",
    "    images, labels = shuffle_data(images, labels)\n",
    "\n",
    "    selected_images = np.zeros((use_num_images, H, W, C))\n",
    "    selected_labels = np.zeros(use_num_images)\n",
    "\n",
    "    for i in range(num_class):\n",
    "        selected_images[i * num_per_class:(i + 1) * num_per_class] = images[labels == i][:num_per_class]\n",
    "        selected_labels[i * num_per_class:(i + 1) * num_per_class] = np.ones((num_per_class)) * i\n",
    "\n",
    "    # Shuffle again\n",
    "    selected_images, selected_labels = shuffle_data(selected_images, selected_labels)\n",
    "\n",
    "    return selected_images, selected_labels\n",
    "\n",
    "def Shrink(X, shrinkArg):\n",
    "    #---- max pooling----\n",
    "    pool = shrinkArg['pool']\n",
    "    # TODO: fill in the rest of max pooling\n",
    "    X=block_reduce(X,(1,pool,pool,1),np.max)\n",
    "    \n",
    "    #---- neighborhood construction\n",
    "    win = shrinkArg['win']\n",
    "    stride = shrinkArg['stride']\n",
    "    pad = shrinkArg['pad']\n",
    "    ch=X.shape[-1]\n",
    "    # TODO: fill in the rest of neighborhood construction\n",
    "    if pad>0:\n",
    "        X=np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'reflect')\n",
    "    X=view_as_windows(X,(1,win,win,ch),(1,stride,stride,ch))\n",
    "    return X.reshape(X.shape[0],X.shape[1],X.shape[2],-1)\n",
    "\n",
    "# example callback function for how to concate features from different hops\n",
    "def Concat(X, concatArg):\n",
    "    return X\n",
    "\n",
    "def get_feat(X, num_layers=3):\n",
    "    output = p2.transform_singleHop(X,layer=0)\n",
    "    if num_layers>1:\n",
    "        for i in range(num_layers-1):\n",
    "            output = p2.transform_singleHop(output, layer=i+1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # ---------- Load MNIST data and split ----------\n",
    "    (x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "    # -----------Data Preprocessing-----------\n",
    "    x_train = np.asarray(x_train,dtype='float32')[:,:,:,np.newaxis]\n",
    "    x_test = np.asarray(x_test,dtype='float32')[:,:,:,np.newaxis]\n",
    "    y_train = np.asarray(y_train,dtype='int')\n",
    "    y_test = np.asarray(y_test,dtype='int')\n",
    "\n",
    "    # if use only 10000 images train pixelhop\n",
    "    x_train_reduced, y_train_reduced = select_balanced_subset(x_train, y_train, use_num_images=N_Train_Reduced)\n",
    "\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0\n",
    "\n",
    "\n",
    "    # -----------Module 1: set PixelHop parameters-----------\n",
    "    # TODO: fill in this part\n",
    "\n",
    "    SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw':False}, \n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'cw':True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'cw':True}]\n",
    "    shrinkArgs = [{'func':Shrink, 'win':5, 'stride':1, 'pad':2,'pool':1}, \n",
    "             {'func': Shrink, 'win':5, 'stride':1, 'pad':0, 'pool':2},\n",
    "             {'func': Shrink, 'win':5, 'stride':1,'pad':0, 'pool':2}]\n",
    "    concatArg = {'func':Concat}\n",
    "    TH1_list=[0.1,0.01,0.005,0.001,0.0005]\n",
    "    acc_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in TH1_list:\n",
    "    # -----------Module 1: Train PixelHop -----------\n",
    "    # TODO: fill in this part\n",
    "    \n",
    "        p2 = Pixelhop(depth=3, TH1=t, TH2=0.001, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg)\n",
    "        p2.fit(x_train_reduced)\n",
    "\n",
    "    \n",
    "    # --------- Module 2: get only Hop 3 feature for both training set and testing set -----------\n",
    "    # you can get feature \"batch wise\" and concatenate them if your memory is restricted\n",
    "    # TODO: fill in this part\n",
    "        split_train_arr=[]\n",
    "    \n",
    "        for i in range(6):\n",
    "                temp=get_feat(x_train[i*BS:(i+1)*BS,:,:,:],3)\n",
    "                split_train_arr.append(temp)\n",
    "            \n",
    "        train_hop3_feats=np.concatenate((split_train_arr[0],split_train_arr[1],split_train_arr[2],split_train_arr[3]\n",
    "                                     ,split_train_arr[4],split_train_arr[5]),axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #train_hop3_feats=get_feat(x_train,3)\n",
    "        test_hop3_feats=get_feat(x_test,3)\n",
    "    ## get model size\n",
    "        h1=get_feat(x_train_reduced,1).shape\n",
    "        h2=get_feat(x_train_reduced,2).shape\n",
    "        h3=get_feat(x_train_reduced,3).shape\n",
    "    \n",
    "        k1=h1[3]\n",
    "        k2=h2[3]\n",
    "        k3=h3[3]\n",
    "    \n",
    "        print('k1: ', k1)\n",
    "        print('k2: ', k2)\n",
    "        print('k3: ', k3)\n",
    "    # --------- Module 2: standardization\n",
    "        STD = np.std(train_hop3_feats, axis=0, keepdims=1)\n",
    "        train_hop3_feats = train_hop3_feats/STD ## size: (60000,1,1,70)\n",
    "    #print(np.shape(train_hop3_feats))\n",
    "    #STDtest = np.std(test_hop3_feats, axis=0, keepdims=1)\n",
    "        test_hop3_feats = test_hop3_feats/STD\n",
    "    \n",
    "    #---------- Module 3: Train XGBoost classifier on hop3 feature ---------\n",
    "        \n",
    "    \n",
    "        clf = xgb.XGBClassifier(n_jobs=-1,\n",
    "                            objective='multi:softprob',\n",
    "                        # tree_method='gpu_hist', gpu_id=None,\n",
    "                            max_depth=6,n_estimators=100,\n",
    "                            min_child_weight=5,gamma=5,\n",
    "                            subsample=0.8,learning_rate=0.1,\n",
    "                            nthread=8,colsamp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee559",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab6e7fd63cfe7c7ccc1577de38240ba1a32fd03de1b002d6532c94e8ce9d2d06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
